{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch import Tensor\n",
    "from einops import rearrange\n",
    "\n",
    "from pl_utils.nn import get_latent_2d_ids, RotaryEmbeddingMultiDimension, apply_rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_utils import init_before_training\n",
    "\n",
    "init_before_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 纯黑和纯白"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class BlackWhiteDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_black, num_white, transform=None):\n",
    "        self.num_black = num_black\n",
    "        self.num_white = num_white\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        # 创建纯黑图\n",
    "        for _ in range(num_black):\n",
    "            img = Image.fromarray(np.zeros((28, 28), dtype=np.uint8))\n",
    "            self.data.append(img)\n",
    "            self.targets.append(10)  # 标签10表示纯黑图\n",
    "\n",
    "        # 创建纯白图\n",
    "        for _ in range(num_white):\n",
    "            img = Image.fromarray(np.ones((28, 28), dtype=np.uint8) * 255)\n",
    "            self.data.append(img)\n",
    "            self.targets.append(11)  # 标签11表示纯白图\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "black_white_dataset = BlackWhiteDataset(1000, 1000, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 MINIST 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_utils.dataset import get_train_val_dataloader\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "dataset = ConcatDataset([train_dataset, test_dataset, black_white_dataset])\n",
    "dataset = ConcatDataset([train_dataset, black_white_dataset])\n",
    "train_loader, _ = get_train_val_dataloader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    test_size=0,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {(train_dataset.data.float() / 255).mean().item()}\")\n",
    "print(f\"Std: {(train_dataset.data.float() / 255).std().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    print(data[0][0][10:20, 10:20])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaLayerNormZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLayerNormZero(nn.Module):\n",
    "    r\"\"\"\n",
    "    改变自 diffusers 库的 AdaLayerNormZero 类。\n",
    "\n",
    "    Norm layer adaptive layer norm zero (adaLN-Zero).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.silu = nn.SiLU()\n",
    "        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=bias)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        emb: torch.Tensor = None,\n",
    "    ):\n",
    "        emb = self.linear(self.silu(emb))\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = emb.chunk(6, dim=1)\n",
    "        hidden_states = self.norm(hidden_states) * (1 + scale_msa[:, None]) + shift_msa[:, None]\n",
    "        return hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaLayerNormContinuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLayerNormContinuous(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.silu = nn.SiLU()\n",
    "        self.linear = nn.Linear(embedding_dim, 2 * embedding_dim, bias=bias)\n",
    "        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        emb: torch.Tensor = None,\n",
    "    ):\n",
    "        emb = self.linear(self.silu(emb))\n",
    "        shift, scale = emb.chunk(2, dim=1)\n",
    "        hidden_states = self.norm(hidden_states) * (1 + scale[:, None]) + shift[:, None]\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import RMSNorm\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"只进行 attention，无 MLP 也无残差。直接返回 attention 结果。\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        head_dim: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        attn_dropout: float,\n",
    "        attn_bias: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.attn_bias = attn_bias\n",
    "\n",
    "        hidden_size = head_dim * num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, num_heads * head_dim, bias=attn_bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, num_kv_heads * head_dim, bias=attn_bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, num_kv_heads * head_dim, bias=attn_bias)\n",
    "        self.q_norm = RMSNorm(head_dim)\n",
    "        self.k_norm = RMSNorm(head_dim)\n",
    "        self.o_proj = nn.Linear(num_heads * head_dim, hidden_size, bias=attn_bias)\n",
    "\n",
    "        self.q_proj_add = nn.Linear(hidden_size, num_heads * head_dim, bias=attn_bias)\n",
    "        self.k_proj_add = nn.Linear(hidden_size, num_kv_heads * head_dim, bias=attn_bias)\n",
    "        self.v_proj_add = nn.Linear(hidden_size, num_kv_heads * head_dim, bias=attn_bias)\n",
    "        self.q_norm_add = RMSNorm(head_dim)\n",
    "        self.k_norm_add = RMSNorm(head_dim)\n",
    "        self.o_proj_add = nn.Linear(num_heads * head_dim, hidden_size, bias=attn_bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        rotary_emb=None,\n",
    "        attn_mask=None,\n",
    "        add_hidden_states=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            hidden_states: [b_size, seq, channel]\n",
    "        \"\"\"\n",
    "        B, S, C = hidden_states.size()\n",
    "        device = hidden_states.device\n",
    "\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "        q = q.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, -1, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, -1, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "\n",
    "        if add_hidden_states is not None:\n",
    "            add_q = self.q_proj_add(add_hidden_states)\n",
    "            add_k = self.k_proj_add(add_hidden_states)\n",
    "            add_v = self.v_proj_add(add_hidden_states)\n",
    "            add_q = add_q.view(B, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            add_k = add_k.view(B, -1, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "            add_v = add_v.view(B, -1, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "            add_q = self.q_norm_add(add_q)\n",
    "            add_k = self.k_norm_add(add_k)\n",
    "            q = torch.cat([add_q, q], dim=2)\n",
    "            k = torch.cat([add_k, k], dim=2)\n",
    "            v = torch.cat([add_v, v], dim=2)\n",
    "\n",
    "        if rotary_emb is not None:\n",
    "            q = apply_rotary_emb(q, rotary_emb)\n",
    "            k = apply_rotary_emb(k, rotary_emb)\n",
    "\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=False,\n",
    "            attn_mask=attn_mask,\n",
    "            enable_gqa=True,\n",
    "        )  # [B, num_heads, seq, head_dim]\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(B, -1, C)  # [B, seq, hidden_size]\n",
    "\n",
    "        if add_hidden_states is not None:\n",
    "            add_hidden_states, hidden_states = (\n",
    "                attn_output[:, : add_hidden_states.size(1)],\n",
    "                attn_output[:, add_hidden_states.size(1) :],\n",
    "            )\n",
    "            add_hidden_states = self.o_proj_add(add_hidden_states)\n",
    "            hidden_states = self.o_proj(hidden_states)\n",
    "\n",
    "            return hidden_states, add_hidden_states\n",
    "\n",
    "        hidden_states = self.o_proj(attn_output)\n",
    "        return hidden_states, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    作为 Transformer 的 MLP 部分。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        mlp_hidden_size_factor: int = 4,\n",
    "        mlp_dropout: float = 0.0,\n",
    "        mlp_bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * mlp_hidden_size_factor, bias=mlp_bias),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(mlp_dropout),\n",
    "            nn.Linear(hidden_size * mlp_hidden_size_factor, hidden_size, bias=mlp_bias),\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_dim=256,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.adanorm = AdaLayerNormZero(transformer_dim)\n",
    "        self.adanorm_add = AdaLayerNormZero(transformer_dim)\n",
    "\n",
    "        self.attn = Attention(**kwargs)\n",
    "\n",
    "        self.mlp = FeedForward(hidden_size=transformer_dim, **kwargs)\n",
    "        self.mlp_add = FeedForward(hidden_size=transformer_dim, **kwargs)\n",
    "\n",
    "        self.attn_norm = nn.LayerNorm(transformer_dim, elementwise_affine=False, eps=1e-6)\n",
    "        self.attn_norm_add = nn.LayerNorm(transformer_dim, elementwise_affine=False, eps=1e-6)\n",
    "        self.mlp_norm = nn.LayerNorm(transformer_dim, elementwise_affine=False, eps=1e-6)\n",
    "        self.mlp_norm_add = nn.LayerNorm(transformer_dim, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        temb,\n",
    "        rotary_emb=None,\n",
    "        attn_mask=None,\n",
    "        add_hidden_states=None,\n",
    "    ):\n",
    "        # AdaNorm\n",
    "        hidden_states = self.attn_norm(hidden_states)\n",
    "        norm_hidden, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adanorm(\n",
    "            hidden_states, emb=temb\n",
    "        )\n",
    "        if add_hidden_states is not None:\n",
    "            add_hidden_states = self.attn_norm_add(add_hidden_states)\n",
    "            norm_add_hidden, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp = self.adanorm_add(\n",
    "                add_hidden_states, emb=temb\n",
    "            )\n",
    "\n",
    "        # attention\n",
    "        attn_output, add_attn_output = self.attn(\n",
    "            norm_hidden,\n",
    "            rotary_emb,\n",
    "            attn_mask=attn_mask,\n",
    "            add_hidden_states=norm_add_hidden if add_hidden_states is not None else None,\n",
    "        )\n",
    "        # residual\n",
    "        hidden_states = hidden_states + attn_output * gate_msa.unsqueeze(1)\n",
    "        # norm\n",
    "        norm_hidden = self.mlp_norm(hidden_states)\n",
    "        norm_hidden = norm_hidden * (1 + scale_mlp[:, None]) + shift_mlp[:, None]\n",
    "        # MLP\n",
    "        mlp_output = self.mlp(norm_hidden)\n",
    "        # residual\n",
    "        hidden_states = hidden_states + mlp_output * gate_mlp.unsqueeze(1)\n",
    "\n",
    "        if add_hidden_states is not None:\n",
    "            # residual\n",
    "            add_hidden_states = add_hidden_states + add_attn_output * c_gate_msa.unsqueeze(1)\n",
    "            # norm\n",
    "            norm_add_hidden = self.mlp_norm_add(add_hidden_states)\n",
    "            norm_add_hidden = norm_add_hidden * (1 + c_scale_mlp[:, None]) + c_shift_mlp[:, None]\n",
    "            # MLP\n",
    "            add_mlp_output = self.mlp_add(norm_add_hidden)\n",
    "            # residual\n",
    "            add_hidden_states = add_hidden_states + add_mlp_output * c_gate_mlp.unsqueeze(1)\n",
    "\n",
    "            return hidden_states, add_hidden_states\n",
    "\n",
    "        return hidden_states, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_utils.nn import TimeStepSinusoidalEmbbedding\n",
    "\n",
    "\n",
    "class Transformer2DModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 16,\n",
    "        *,\n",
    "        transformer_dim: int = 256,\n",
    "        num_layers: int = 12,\n",
    "        time_emb_dim=128,\n",
    "        class_emb_dim=128,\n",
    "        num_classes=10,\n",
    "        head_dim: int = 64,\n",
    "        rope_theta: int = 10000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.in_proj = nn.Linear(in_channels, transformer_dim)\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            TimeStepSinusoidalEmbbedding(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, transformer_dim),\n",
    "        )\n",
    "        self.class_embed = nn.Sequential(\n",
    "            TimeStepSinusoidalEmbbedding(class_emb_dim, 10000),\n",
    "            nn.Linear(class_emb_dim, class_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(class_emb_dim, transformer_dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embed = RotaryEmbeddingMultiDimension(rope_theta, dim=[head_dim // 2, head_dim // 2])\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(transformer_dim=transformer_dim, head_dim=head_dim, **kwargs)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.out_adanorm = AdaLayerNormContinuous(transformer_dim)\n",
    "        self.out_proj = nn.Linear(transformer_dim, in_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        latents,\n",
    "        t,\n",
    "        class_idx,\n",
    "        pos_ids,\n",
    "        attn_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            latents: [b, seq, c]\n",
    "            t: [b]\n",
    "            class_idx: [b]\n",
    "            pos_ids: [seq, 2]\n",
    "        \"\"\"\n",
    "        device = latents.device\n",
    "        B, S, C = latents.size()\n",
    "\n",
    "        hidden_states = self.in_proj(latents)\n",
    "\n",
    "        # time & class embedding\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0).repeat(B)\n",
    "        t_emb = self.time_embed(t.to(device))  # [b, transformer_dim]\n",
    "        if class_idx.dim() == 0:\n",
    "            class_idx = class_idx.unsqueeze(0).repeat(B)\n",
    "        class_emb = self.class_embed(class_idx.to(device))  # [b, transformer_dim]\n",
    "\n",
    "        # temb\n",
    "        temb = t_emb\n",
    "        # add_hidden_states\n",
    "        add_hidden_states = class_emb.unsqueeze(1)\n",
    "\n",
    "        # rotary emb\n",
    "        if add_hidden_states is not None:\n",
    "            add_pos_ids = torch.ones(add_hidden_states.size(1), 2).to(pos_ids.device)\n",
    "            pos_ids = torch.cat([pos_ids, add_pos_ids], dim=0)  # [seq + n, 2]\n",
    "        rotary_emb = self.pos_embed(pos_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            hidden_states, add_hidden_states = block(\n",
    "                hidden_states=hidden_states,\n",
    "                temb=temb,\n",
    "                rotary_emb=rotary_emb,\n",
    "                attn_mask=attn_mask,\n",
    "                add_hidden_states=add_hidden_states,\n",
    "            )\n",
    "\n",
    "        hidden_states = self.out_adanorm(hidden_states, emb=temb)\n",
    "        latents = self.out_proj(hidden_states)\n",
    "\n",
    "        return latents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiffusionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shift(\n",
    "    image_seq_len,\n",
    "    base_seq_len: int = 256,\n",
    "    max_seq_len: int = 4096,\n",
    "    base_shift: float = 0.5,\n",
    "    max_shift: float = 1.15,\n",
    "):\n",
    "    \"\"\"\n",
    "    取自 diffusers 库的 calculate_shift 函数。\n",
    "\n",
    "    用于动态决定时间步的偏移程度，使其与分辨率匹配。\n",
    "\n",
    "    本质是线性函数。\n",
    "    \"\"\"\n",
    "    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n",
    "    b = base_shift - m * base_seq_len\n",
    "    mu = image_seq_len * m + b\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.schedulers.scheduling_flow_match_euler_discrete import (\n",
    "    FlowMatchEulerDiscreteScheduler,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from einops.layers.torch import Rearrange\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_channels: int = 1,\n",
    "        patch_size: int = 2,\n",
    "        num_train_timesteps: int = 1000,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_channels = img_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.num_train_timesteps = num_train_timesteps\n",
    "\n",
    "        mu = calculate_shift((28 // patch_size) ** 2)\n",
    "        self.train_scheduler = FlowMatchEulerDiscreteScheduler(\n",
    "            num_train_timesteps=num_train_timesteps,\n",
    "            use_dynamic_shifting=True,\n",
    "        )\n",
    "        self.train_scheduler.set_timesteps(num_train_timesteps, mu=mu)\n",
    "        self.infer_scheduler = FlowMatchEulerDiscreteScheduler(\n",
    "            use_dynamic_shifting=True,\n",
    "        )\n",
    "\n",
    "        self.transformer_model = Transformer2DModel(\n",
    "            in_channels=img_channels * patch_size**2,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def img_normalize(self, img):\n",
    "        return (img - 0.5) / 0.3081\n",
    "\n",
    "    def img_denormalize(self, img):\n",
    "        return img * 0.3081 + 0.5\n",
    "\n",
    "    def pack_latent(self, latents):\n",
    "        return rearrange(\n",
    "            latents,\n",
    "            \"b c (h p1) (w p2) -> b (h w) (c p1 p2)\",\n",
    "            p1=self.patch_size,\n",
    "            p2=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def unpack_latent(self, latents, height, width):\n",
    "        return rearrange(\n",
    "            latents,\n",
    "            \"b (h w) (c p1 p2) -> b c (h p1) (w p2)\",\n",
    "            h=height,\n",
    "            w=width,\n",
    "            p1=self.patch_size,\n",
    "            p2=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def get_train_loss(self, samples, t, class_idx):\n",
    "        device = samples.device\n",
    "        B, C, H, W = samples.size()\n",
    "\n",
    "        # prepare latent variables\n",
    "        latents = self.img_normalize(samples)\n",
    "        latents = self.pack_latent(latents)\n",
    "        latents_pos_ids = get_latent_2d_ids(H // self.patch_size, W // self.patch_size)\n",
    "        noise = torch.randn_like(latents).to(device)\n",
    "\n",
    "        ### offset noise\n",
    "        # noise += 0.1 * torch.randn(noise.size(0), 1, noise.size(2)).to(device)\n",
    "        ### offset noise\n",
    "\n",
    "        ### immiscible diffusion\n",
    "        # a = latents.view(B, -1)\n",
    "        # b = noise.view(B, -1)\n",
    "        # dist = torch.cdist(a, b)\n",
    "        # _, idx = linear_sum_assignment(dist.cpu())\n",
    "        # noise = noise[torch.from_numpy(idx).to(device)]\n",
    "        ### immiscible diffusion\n",
    "\n",
    "        # timesteps to sigmas\n",
    "        t = -t - 1\n",
    "        sigmas = self.train_scheduler.sigmas[t.to(\"cpu\")][:, None, None].to(device)\n",
    "\n",
    "        # get noised latents\n",
    "        noised_latents = (1.0 - sigmas) * latents + sigmas * noise\n",
    "        # get pred latents\n",
    "        pred_latents = self.transformer_model(\n",
    "            noised_latents,\n",
    "            t,\n",
    "            class_idx,\n",
    "            pos_ids=latents_pos_ids,\n",
    "        )\n",
    "        # get loss\n",
    "        loss = F.mse_loss(noise - latents, pred_latents)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, samples, t, class_idx):\n",
    "        return self.get_train_loss(samples, t, class_idx)\n",
    "\n",
    "    def sample_from_scratch(\n",
    "        self,\n",
    "        batch_size,\n",
    "        height,\n",
    "        width,\n",
    "        class_idx,\n",
    "        num_timesteps,\n",
    "        latents=None,\n",
    "    ):\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # prepare latent variables\n",
    "        if latents is None:\n",
    "            latents = torch.randn(batch_size, self.img_channels, height, width)\n",
    "        latents = latents.to(device)\n",
    "        latents = self.pack_latent(latents)\n",
    "        latents_pos_ids = get_latent_2d_ids(height // self.patch_size, width // self.patch_size)\n",
    "\n",
    "        # timesteps\n",
    "        mu = calculate_shift(latents.size(1))\n",
    "        self.infer_scheduler.set_timesteps(num_timesteps, mu=mu, device=device)\n",
    "        timesteps_list = self.infer_scheduler.timesteps\n",
    "\n",
    "        # denoising loop\n",
    "        for timestep in tqdm(timesteps_list, leave=False):\n",
    "            with torch.no_grad():\n",
    "                noise_pred = self.transformer_model(\n",
    "                    latents=latents,\n",
    "                    t=timestep.to(latents.device),\n",
    "                    class_idx=class_idx.to(latents.device),\n",
    "                    pos_ids=latents_pos_ids,\n",
    "                )\n",
    "            latents = self.infer_scheduler.step(noise_pred, timestep, latents, return_dict=False)[0]\n",
    "\n",
    "        # unpack latents\n",
    "        latents = self.unpack_latent(latents, height // self.patch_size, width // self.patch_size)\n",
    "\n",
    "        # denormalize\n",
    "        samples = self.img_denormalize(latents)\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_utils import BaseModule\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import os\n",
    "\n",
    "\n",
    "class LightningModel(BaseModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: DiffusionModel,\n",
    "        learning_rate_config,\n",
    "        training_config,\n",
    "        ema_update_callback=None,\n",
    "    ):\n",
    "        super().__init__(model, learning_rate_config, training_config)\n",
    "        self.ema_update_callback = ema_update_callback\n",
    "\n",
    "        self.model.apply(self._init_weight)\n",
    "\n",
    "    def forward(self, batch_size=1, height=28, width=28, class_idx=0, num_timesteps=10):\n",
    "        return self.model(batch_size, height, width, class_idx, num_timesteps)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        samples, class_idx = batch\n",
    "        batch_size = samples.size(0)\n",
    "\n",
    "        timesteps = torch.normal(0, 1, (batch_size,)).sigmoid()\n",
    "        timesteps = (timesteps * (self.model.num_train_timesteps - 1)).long()\n",
    "\n",
    "        loss = self.model(samples, timesteps, class_idx)\n",
    "        grad_norm = nn.utils.clip_grad_norm_(self.model.parameters(), 2.0)\n",
    "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/grad_norm', grad_norm, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        if self.ema_update_callback:\n",
    "            self.ema_update_callback()\n",
    "        pass\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        label_indices = torch.arange(12)\n",
    "        if not hasattr(self, \"tmp_latents\"):\n",
    "            self.tmp_latents = torch.randn(\n",
    "                1,\n",
    "                self.model.img_channels,\n",
    "                28,\n",
    "                28,\n",
    "            ).repeat(12, 1, 1, 1)\n",
    "        self.model.eval()\n",
    "        samples = self.model.sample_from_scratch(\n",
    "            12, 28, 28, label_indices, 20, latents=self.tmp_latents\n",
    "        )\n",
    "\n",
    "        images = (samples.clamp(0, 1) * 255).to(torch.uint8)\n",
    "        grid = make_grid(images, nrow=4, pad_value=255, value_range=(0, 255))\n",
    "        os.makedirs('./training_images', exist_ok=True)\n",
    "        save_image(grid / 255, f'./training_images/epoch_{self.current_epoch}.png')\n",
    "\n",
    "    def _init_weight(self, module):\n",
    "        std = 0.002\n",
    "        if isinstance(module, (nn.Linear)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, (nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置与实例化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_utils import LearningRateConfig, TrainingConfig\n",
    "\n",
    "learning_rate_config = LearningRateConfig(\n",
    "    lr_warmup_steps=1000,\n",
    "    # lr_warmup_steps=0.,\n",
    "    lr_initial=1e-6,\n",
    "    lr_max=2e-4,\n",
    "    lr_end=2e-4,\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    optimizer='adamw',\n",
    "    optimizer_args={\n",
    "        # 'betas': (0.9, 0.999),\n",
    "        # 'weight_decay': 1e-4,\n",
    "        \"fused\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "layer_config = {\n",
    "    # DiffusionModel\n",
    "    \"img_channels\": 1,\n",
    "    \"patch_size\": 2,\n",
    "    \"num_train_timesteps\": 1000,\n",
    "    # Transformer2DModel\n",
    "    \"transformer_dim\": 256,\n",
    "    \"num_layers\": 3,\n",
    "    \"time_emb_dim\": 256,\n",
    "    \"class_emb_dim\": 256,\n",
    "    \"num_classes\": 12,\n",
    "    \"head_dim\": 64,\n",
    "    \"rope_theta\": 100,\n",
    "    # TransformerBlock\n",
    "    # Attention\n",
    "    \"num_heads\": 4,\n",
    "    \"num_kv_heads\": 1,\n",
    "    \"attn_dropout\": 0.0,\n",
    "    \"attn_bias\": True,\n",
    "    # FeedForward\n",
    "    \"mlp_hidden_size_factor\": 4,\n",
    "    \"mlp_dropout\": 0.0,\n",
    "    \"mlp_bias\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModel(**layer_config).to('cuda')\n",
    "\n",
    "pl_model = LightningModel(\n",
    "    model,\n",
    "    learning_rate_config,\n",
    "    training_config,\n",
    "    ema_update_callback=None,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    fullgraph=False,\n",
    "    dynamic=False,\n",
    "    options={\n",
    "        \"shape_padding\": True,\n",
    "        \"max_autotune\": True,\n",
    "        # \"triton.cudagraphs\": True,\n",
    "        # \"trace.graph_diagram\": True,\n",
    "    },\n",
    ")\n",
    "# model.compile(fullgraph=True, dynamic=False)\n",
    "# model.compile(dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正式训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=0,\n",
    "    # monitor=\"val/reconstruct_loss\",\n",
    "    # mode=\"min\",\n",
    "    # dirpath=r\"/mnt/e/vae_models\",\n",
    "    # save_weights_only=True,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=False,\n",
    "    accelerator='gpu',\n",
    "    max_epochs=300,\n",
    "    precision='bf16-mixed',\n",
    "    # precision='32',\n",
    "    log_every_n_steps=4,\n",
    "    default_root_dir=\"./\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    num_sanity_val_steps=0,\n",
    "    # fast_dev_run=True,\n",
    ")\n",
    "\n",
    "trainer.fit(pl_model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
